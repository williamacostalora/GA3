---
title: "Your Title"
subtitle: "STAT 253: Statistical Machine Learning"
date: today
author: "Your Names"
format:
  html:
    toc: true
    toc-depth: 3
    embed-resources: true
    code-tools: true
---


<!-- Your report should follow the format specified in the Group Assignment 3 Instructions. Please review that document carefully! -->





```{r}
#| include: false
# Load packages 
library(tidyverse)
library(tidymodels)
library(cluster)      # to build the hierarchical clustering algorithm
library(factoextra)
library(reshape2)

```


# Research Goals
We are Glee Cast fans who host a weekly show at the WMCN radio station. Before approving our programming, our manager asked for a review successful tracks at the station. Our goal is to summarize and take a closer look at the station hits, audience responses, and identify any song features of the songs that made it to the "Billboard Hot 100".




# Data
In order to understand the popularity of Glee Cast’s songs we retrieved data from the the Billboard Top 100 list that has been processed, arranged and cleaned in 2021 by participants of the Tidy Tuesday challenge, an event at the Macalester College Math Computer Science and Statistics Department. 

The song's popularity record data was originally retrieved from Data.World. As background context, the “Billboard Hot 100” is a record chart for songs, regardless of the artist, that has been published weekly since 1958 by the Billboard magazine based on the digital and physical sales, radio airplay and online streaming in the U.S. 

The retrieved dataset consists of 243,313 observations representing songs by thousands of artists from years prior to 2021, while also accounting for 15 variables including Spotify popularity, billboard weeks and song features such as danceability, energy, loudness, liveness among others. For our research purpose we selected the Glee Cast as our main artist focus which included an overall of 181 songs. 

Since our goal is exploration of the features, we use an unsupervised learning approach. Therefore we do not set a determined outcome ‘y’ but rather we try to take a look at the internal structure to understand the patterns. 

```{r}
#| message: false
#| warning: false
#| include: false

# read in data
music <- read.csv("https://bcheggeseth.github.io/253_spring_2024/data/billboard.csv")

# Check out artists with at least 40 songs
music %>% 
  count(performer) %>% 
  filter(n >= 40) %>% 
  select(performer)

# Pick just one of these artists to study
my_artist <- music %>% 
  filter(performer == "Glee Cast") %>% 
  select(-performer) %>% 
  group_by(song) %>%       # The last rows deal w songs that appear more than once
  slice_sample(n = 1) %>% 
  ungroup()

my_artist <- my_artist %>% 
  column_to_rownames("song")

my_artist %>% 
  mutate(mode = as.factor(mode)) %>%
  mutate(key = as.factor(key)) %>%
  mutate(time_signature = as.factor(time_signature))


```

```{r}
#| message: false
#| warning: false
#| echo: false

# visualization
scaled_artist <- my_artist %>%
  mutate(across(everything(), scale))

scaled_artist <- scaled_artist %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "value")

# Boxplot of scaled variables
ggplot(scaled_artist, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(alpha = 0.7, outlier.shape = 16, outlier.alpha = 0.5) +
  scale_fill_viridis_d(option = "plasma") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.major.x = element_blank()
  ) +
  labs(title = "Distribution of Scaled Audio Features",
       subtitle = "Standardized values (z-scores) for Glee Cast songs",
       x = "Audio Features",
       y = "Scaled Value (z-score)")
```

To show all the features, their scale and variance, we decided to include a boxplot of the scaled predictors. In most of the unsupervised learning techniques such as hierarchical clustering or PCA - which we will be using in this analysis - features need to be standardized in order to prevent one of features with a bigger scale to have more weight on the data exploration. Here we can see that all features mean lie in the same line (0), however their variance and distribution looks different for all, allowing to overall already check for some feature that might have the highest variance for PCA analysis.  


# Cluster Analysis

## Implementation

We selected hierarchical clustering with complete linkage for this analysis because:

1. Mixed data types: The dataset contains quantitative variables (duration_ms, danceability, energy, etc.) and categorical variables (mode, key, time_signature). Hierarchical clustering with the Gower distance metric handles this mixture by computing appropriate distances for each variable type.

2. Exploratory flexibility: Hierarchical clustering does not require specifying the number of clusters upfront, allowing us to explore the dendrogram and use methods to experminent with the number of k.

3. Complete linkage: We chose complete linkage because it measures the maximum distance between points in different clusters, which tends to produce more compact, well-separated clusters. This is particularly useful for identifying distinct song profiles where we want clear boundaries between cluster types.

To determine the optimal number of clusters, we used the elbow method in the Total Within-Cluster Sum of Squares Plot below:

The elbow on the curve appears around k = 4 where the curve starts to flatten out significantly. There are sharp drops from k=1 to k=3, but after k=4, the improvements become much smaller and the curve levels off. This indicates that k=4 captures most of the meaningful structure in the data, and adding more clusters beyond this point doesn't substantially improve the clustering quality.


<!-- don't modify the formatting below here -->
See code below for full details.

<details>
<summary>View Code</summary>

```{r}
#| message: false
#| warning: false

# Include all clustering code in here.
# Make sure to include comments explaining what your code does.

# Scenario 2: AT LEAST 1 feature x is a FACTOR (categorical)
# Use either a "complete", "single", "average", or "centroid" linkage
hier_model <- hclust(daisy(my_artist, metric = "gower"), method = "complete")

# Heat maps: ordered by the id variable (not clustering)
heatmap(scale(data.matrix(my_artist)), Colv = NA, Rowv = NA)

# Heat maps: ordered by dendrogram / clustering
heatmap(scale(data.matrix(my_artist)), Colv = NA)

# Build hierarchical clustering model using Gower distance
# Gower distance handles mixed data types (numeric + categorical)
hier_model <- hclust(daisy(my_artist, metric = "gower"), method = "complete")

# Visualize the full dendrogram
fviz_dend(hier_model, cex = 0.5, main = "Complete Dendrogram of Glee Cast Songs")

# Calculate total within-cluster sum of squares for k=1 to 10
# Using cutree to cut the hierarchical tree at different heights
ss_data <- tibble(k = 1:10) %>%
  mutate(tot_withinss = map_dbl(k, ~ {
    # Cut tree at k clusters
    clusters <- cutree(hier_model, k = .x)
    # Calculate distances
    dist_matrix <- daisy(my_artist, metric = "gower")
    # Calculate total within-cluster sum of squares
    sum(sapply(unique(clusters), function(cluster) {
      cluster_points <- which(clusters == cluster)
      if(length(cluster_points) > 1) {
        cluster_dist <- as.matrix(dist_matrix)[cluster_points, cluster_points]
        sum(cluster_dist^2) / (2 * length(cluster_points))
      } else {
        0
      }
    }))
  }))

# Plot the elbow curve
ggplot(ss_data, aes(x = k, y = tot_withinss)) +
  geom_line() +
  geom_point(size = 3) +
  labs(title = "Total Within-Cluster Sum of Squares by K",
       x = "Number of Clusters (k)",
       y = "Total Within-Cluster Sum of Squares") +
  theme_minimal()

# Create final clustering with k=4
cluster_data <- my_artist %>% 
  mutate(cluster = as.factor(cutree(hier_model, k = 4)))

# Visualize clusters on dendrogram
fviz_dend(hier_model, k = 4, cex = 0.35, 
          main = "Dendrogram with 4 Clusters Highlighted",
          palette = "jco")

# Check cluster sizes
cluster_sizes <- cluster_data %>% 
  count(cluster) %>%
  arrange(desc(n))

print(cluster_sizes)
 
```

</details>


## Insights

The hierarchical clustering identified four musical profiles in the 181 Glee Cast songs which are visible in the colored dendogram from the previous section. Cluster 1 (n=113) is the largest group with moderate energy (0.68), danceability (0.55), and valence (0.51). The energy vs danceability scatter plot shows these songs spread across the middle range. These mainstream pop songs average 3.6 minutes with low acousticness (0.22) and are entirely in major keys (mode=1.00). Cluster 2 (n=42) has the highest energy (0.81) and danceability (0.64),  visible in the upper-right corner of the scatter plot. The bar chart shows peaks in energy and danceability for this cluster. With high valence (0.62) and low acousticness (0.11), these dance tracks are the loudest (-4.68 dB) and fastest (124 BPM), averaging 3.5 minutes.

Cluster 3 (n=7) and Cluster 4 (n=19) contain lower-energy songs, positioned in the lower-left region of the energy vs danceability plot. Cluster 3 has moderate-low energy (0.39) and danceability (0.37) with acousticness at 0.49, visible in the bar chart. The valence vs acousticness plot shows this cluster has the lowest valence (0.23). The duration boxplot shows these songs average 3.9 minutes, the longest duration. Cluster 4 has the lowest energy (0.24) and highest acousticness (0.86), appearing as the tallest bar in the acousticness section. With low valence (0.29), this cluster has the shortest duration (2.9 minutes) in the boxplot, and is the quietest (-10.18 dB) and slowest (107 BPM). Both clusters use only minor keys (mode=0.00), contrasting with Cluster 1's major keys.

The visualizations confirm four distinct performance styles: mainstream pop (Cluster 1), high-energy dance numbers (Cluster 2), acoustic-emotional pieces (Cluster 3), and intimate ballads (Cluster 4). The scatter plots show clear separation between high-energy and low-energy clusters, while the bar chart highlights acousticness differences between produced and acoustic performances. The averaged features for each cluster assignment also follow this trend
```{r}
#| echo: false
# Visualization 1: Cluster characteristics across key audio features
cluster_summary <- cluster_data %>%
  group_by(cluster) %>%
  summarise(
    avg_energy = mean(energy),
    avg_danceability = mean(danceability),
    avg_valence = mean(valence),
    avg_acousticness = mean(acousticness),
    avg_speechiness = mean(speechiness),
    count = n()
  ) %>%
  pivot_longer(cols = starts_with("avg_"), 
               names_to = "feature", 
               values_to = "value") %>%
  mutate(feature = str_remove(feature, "avg_"))

ggplot(cluster_summary, aes(x = feature, y = value, fill = cluster)) +
  geom_col(position = "dodge") +
  labs(title = "Average Audio Feature Values by Cluster",
       subtitle = "Comparing musical characteristics across the 4 identified clusters",
       x = "Audio Feature",
       y = "Average Value",
       fill = "Cluster") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# Visualization 2: Energy vs Danceability scatter plot
ggplot(cluster_data, aes(x = danceability, y = energy, color = cluster, size = valence)) +
  geom_point(alpha = 0.6) +
  labs(title = "Song Clusters: Energy vs Danceability",
       subtitle = "Clear separation between high-energy dance tracks, pop anthems, and mellow ballads",
       x = "Danceability",
       y = "Energy",
       color = "Cluster",
       size = "Valence") +
  theme_minimal()


# Visualization 3: Valence vs Acousticness
ggplot(cluster_data, aes(x = acousticness, y = valence, color = cluster)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "Emotional Tone: Valence vs Acousticness by Cluster",
       subtitle = "Ballads show lower valence with higher acousticness, while dance tracks are upbeat",
       x = "Acousticness",
       y = "Valence (Musical Positivity)",
       color = "Cluster") +
  theme_minimal()

# Visualization 4: Duration distribution by cluster
ggplot(cluster_data, aes(x = cluster, y = duration_ms/60000, fill = cluster)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(title = "Song Duration Distribution by Cluster",
       subtitle = "Cluster 4 shows greater variability, including extended theatrical performances",
       x = "Cluster",
       y = "Duration (minutes)",
       fill = "Cluster") +
  theme_minimal()

# Getting mean features by cluster for ALL audio features 
cluster_summary <- cluster_data %>%
  group_by(cluster) %>%
  summarise(
    count = n(),
    avg_duration_min = mean(duration_ms) / 60000,
    avg_danceability = mean(danceability),
    avg_energy = mean(energy),
    avg_key = mean(as.numeric(key)),
    avg_loudness = mean(loudness),
    avg_mode = mean(as.numeric(mode)),
    avg_speechiness = mean(speechiness),
    avg_acousticness = mean(acousticness),
    avg_instrumentalness = mean(instrumentalness),
    avg_liveness = mean(liveness),
    avg_valence = mean(valence),
    avg_tempo = mean(tempo),
    avg_time_signature = mean(as.numeric(time_signature))
  ) %>%
  arrange(cluster)

# Display as formatted table
knitr::kable(cluster_summary, 
             digits = 2,
             col.names = c("Cluster", "Count", "Duration (min)", "Danceability", 
                          "Energy", "Key", "Loudness", "Mode", "Speechiness",
                          "Acousticness", "Instrumentalness", "Liveness", 
                          "Valence", "Tempo", "Time Sig"),
             caption = "Average Audio Features by Cluster (All Features)")

```


# Dimension Reduction

## Implementation

PCA is a tool that helps simplify a big list of song characteristics by turning them into a few main themes or patterns. Instead of looking at every feature separately, like danceability, energy, loudness, and acousticness, PCA groups related features together into new combined categories.

Each of these new categories is called a principal component. The first one (PC1) represents the strongest overall pattern in the songs. The second one (PC2) represents the next most important pattern, and each one after that captures a smaller part of what makes the songs different from each other.

The code below shows how strongly each song characteristic contributes to each principal component, and the percentage of the difference between the songs that is explained by each principal component.


<!-- don't modify the formatting below here -->
See code below for full details.

<details>
<summary>View Code</summary>

```{r}
#| message: false
#| warning: false

# Include all dimension reduction code in here.
# Make sure to include comments explaining what your code does.

# scale = TRUE, center = TRUE first standardizes the features
pca_results <- prcomp(my_artist, scale = TRUE, center = TRUE)

# Get the loadings which define the PCs
pca_results %>% 
  pluck("rotation")

#Measure information captured by each PC
pca_results %>% 
  tidy(matrix = "eigenvalues")

```

</details>



## Insights

Visualization 1 below shows in detail how much of each variable is used in PC1 through PC5. Below are explanations of the first 2 PCs.

PC1: separates songs based on how energetic and “produced” they sound (Energy vs. Acousticness). On the higher side of PC1 we see songs with high danceability, energy, loudness, valence. On the negative side of PC1 we see songs that are  quieter, more acoustic, more speech-like or instrumental.PC1 basically captures the difference between high-energy pop numbers and softer, more acoustic songs in the Glee Cast catalog.

PC2: separates songs based on how well the songs performed commercially. Songs that spent more weeks on the Billboard charts or have higher Spotify popularity end up higher on PC2. PC2 basically distinguishes between songs that were super successful and popular from those that were not as much of a hit.


PC1 explains about 25% of all the differences between songs (variance), and PC2 explains about 10%. The next three components each add small chunks of information (7-9%). After the PC5, the remaining components contribute very little additional information while making the analysis even more complex by adding extra dimensions. While the largest drop in importance occurs after the first two components, as seen in Visualization 2, we decided to keep five PCs because they provide a more complete picture of how the songs differ from one another. Including five components makes the analysis a bit more complicated, but it allows us to retain more information of the meaningful musical variation in the data.

By keeping five principal components, we keep about 60% of all the information in the original data, as seen in Visualization 3. This means we’re still capturing most of the major musical patterns, such as energy level, acousticness, popularity, and other qualities, while reducing the dataset from 15 separate features down to just 5 summarized dimensions.


```{r}
#| echo: false
# put code for visualizations 
# Load package for tidy table

# PCA Visualization 1: Plotting loadings for first 5 PCs
melt(pca_results$rotation[, 1:5]) %>% 
  ggplot(aes(x = Var1, y = value, fill = Var1)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ Var2) + 
    labs(y = "loadings", x = "original features", fill = "original features")+
    theme(axis.text.x  = element_text(size = 7, angle = 45, hjust = 1) )


# PCA Visualization 2: Scree Plot (% of variance explained by each PC)
pca_results %>% 
  tidy(matrix = "eigenvalues") %>% 
  ggplot(aes(y = percent, x = PC)) + 
    geom_point(size = 2) + 
    geom_line() + 
    labs(y = "% of variance explained")


# PCA Visualization 3: Cumulative % of variance explained by each PC
pca_results %>% 
  tidy(matrix = "eigenvalues") %>% 
  rbind(0) %>% 
  ggplot(aes(y = cumulative, x = PC)) + 
    geom_point(size = 2) + 
    geom_line() + 
    labs(y = "CUMULATIVE % of variance explained")

```



# Conclusion

## Key Takeaways

### Clustering

Clustering analysis of this data identified 4 key clusters across songs by the Glee Cast.

- Cluster 1: This is the largest cluster, representing mainstream pop songs with moderate energy and danceability. These songs also have moderate positivity with low acousticness, which indicate that they are produced, studio-polished tracks. Duration for these songs is about 3.5 minutes, a typical length for radio-friendly pop songs.

- Cluster 2: This is the second largest cluster, representing high-energy songs with strong danceability and high energy levels. These songs also display positive-sounding trends with low acousticness and moderate speehiness. These qualities suggest that electronic production with some rhythmic vocal elements are present.

- Cluster 3: This is the second smallest cluster, representing longer songs with a duration range of about 3.5-4.5 minutes. These songs have moderate energy and danceability, with high acousticness overall. Additionally, these songs have a noticeably wide valence, indicating emotional diversity across the cluster.

- Cluster 4: This is the smallest cluster, representing low energy and highly acoustic songs. These songs are pretty variable in valence and song length, ranging from 2-3.5 minutes. Thus, this cluster is likely made up of songs with diverse performance types.

### PCA

Principal component analysis of this data suggests the use of 5 PCs for this dataset. These 5 PCs maintain about 60% of the original data, reducing the 15 initial features down to 5 combined dimensions. Although this makes the analysis a bit more complicated, more information is retained by using 5 PCs. The first 2 PCs- which cumulatively explain about 35% of variability -are defined as follows:

- PC1: This PC separates songs based on their energy and acousticness levels, thus providing insight into how energetic and produced they sound. The positive side of the PC groups together songs with high danceability, energy, loudness, and valence. Conversely, the negative side of the PC includes songs that are quieter, more acoustic, and more speech-like or instrumental.

- PC2: This PC separates songs based on their time spent on the Billboard charts and Spotify popularity, thus providing insight into how well they performed commercially. The positive side of the PC groups together songs that spent more time on the Billboard charts and have a higher Spotify popularity, whereas those on the negative side are generally less popular.


## Limitations

Clustering: Computationally expensive, greedy.

PCA: Features no longer have their original meanings, thus restricting interpretability.

In general, the results from these modelling algorithms are difficult to interpret. Further, these algorithms only provide insight into the general structure within the data (unsupervised learning), and cannot be used to make predictions for external data points.


# Contributions
Ana focused on PCA code, exploration, implementation and insights. She wrote all analysis of the PCA.
  
Lucia wrote the Research Goals and Data sections, as well as the initial exploration of the features through the boxplot. 
  
William focused on Clustering code. He wrote the implementation and insights on Clustering.
  
Georgia also ran PCA and Clustering code for confirmation, she also wrote the Conclusion.
  
We all helped reviewing and commenting each other's sections for improvement.



# Appendix

```{r}
#| eval: false
# put code for any other methods or visualizations that you considered here
# use comments to explain what your code is doing
```

